{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 1s 205us/step - loss: 55.7844 - val_loss: 53.4170\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 51.1918 - val_loss: 53.1082\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 50.4111 - val_loss: 53.0058\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 49.5170 - val_loss: 52.9012\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 48.3962 - val_loss: 52.7497\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 47.0008 - val_loss: 52.5026\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 1s 114us/step - loss: 45.3462 - val_loss: 52.2649\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 43.5073 - val_loss: 51.8787\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 1s 115us/step - loss: 41.5978 - val_loss: 51.4884\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 1s 123us/step - loss: 39.7312 - val_loss: 51.2421\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 38.0486 - val_loss: 51.1215\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 36.4517 - val_loss: 51.0811\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 34.9536 - val_loss: 51.4149\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 1s 114us/step - loss: 33.4443 - val_loss: 51.6851\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 32.0138 - val_loss: 52.2113\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 30.5771 - val_loss: 52.8059\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 29.1828 - val_loss: 53.4154\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 27.8197 - val_loss: 54.0617\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 26.5073 - val_loss: 54.6792\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 1s 127us/step - loss: 25.1907 - val_loss: 55.4354\n",
      "Epoch 21/100\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 23.9524 - val_loss: 56.2376\n",
      "Epoch 22/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 22.7638 - val_loss: 57.2691\n",
      "Epoch 23/100\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 21.6320 - val_loss: 57.7836\n",
      "Epoch 24/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 20.5068 - val_loss: 58.4806\n",
      "Epoch 25/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 19.4194 - val_loss: 59.1944\n",
      "Epoch 26/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 18.3979 - val_loss: 59.9817\n",
      "Epoch 27/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 17.4612 - val_loss: 60.6788\n",
      "Epoch 28/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 16.5382 - val_loss: 61.5882\n",
      "Epoch 29/100\n",
      "6000/6000 [==============================] - 1s 123us/step - loss: 15.7289 - val_loss: 61.9308\n",
      "Epoch 30/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 14.8735 - val_loss: 62.7532\n",
      "Epoch 31/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 14.0910 - val_loss: 63.4649\n",
      "Epoch 32/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 13.3539 - val_loss: 64.2772\n",
      "Epoch 33/100\n",
      "6000/6000 [==============================] - 1s 123us/step - loss: 12.6229 - val_loss: 64.8858\n",
      "Epoch 34/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 12.0226 - val_loss: 65.6345\n",
      "Epoch 35/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 11.3944 - val_loss: 66.3854\n",
      "Epoch 36/100\n",
      "6000/6000 [==============================] - 1s 127us/step - loss: 10.8145 - val_loss: 66.9274\n",
      "Epoch 37/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 10.2521 - val_loss: 67.4533\n",
      "Epoch 38/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 9.7494 - val_loss: 68.3783\n",
      "Epoch 39/100\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 9.2458 - val_loss: 68.7909\n",
      "Epoch 40/100\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 8.7389 - val_loss: 69.2584\n",
      "Epoch 41/100\n",
      "6000/6000 [==============================] - 1s 128us/step - loss: 8.3307 - val_loss: 69.6360\n",
      "Epoch 42/100\n",
      "6000/6000 [==============================] - 1s 126us/step - loss: 7.8934 - val_loss: 70.2535\n",
      "Epoch 43/100\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 7.5002 - val_loss: 70.6992\n",
      "Epoch 44/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 7.0911 - val_loss: 71.3463\n",
      "Epoch 45/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 6.7732 - val_loss: 71.8174\n",
      "Epoch 46/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 6.4323 - val_loss: 72.2378\n",
      "Epoch 47/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 6.1398 - val_loss: 72.7225\n",
      "Epoch 48/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 5.8213 - val_loss: 73.2125\n",
      "Epoch 49/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 5.5163 - val_loss: 73.6476\n",
      "Epoch 50/100\n",
      "6000/6000 [==============================] - 1s 123us/step - loss: 5.2584 - val_loss: 74.0128\n",
      "Epoch 51/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 5.0292 - val_loss: 74.4564\n",
      "Epoch 52/100\n",
      "6000/6000 [==============================] - 1s 113us/step - loss: 4.7746 - val_loss: 75.0315\n",
      "Epoch 53/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 4.5423 - val_loss: 75.2397\n",
      "Epoch 54/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 4.3108 - val_loss: 75.4895\n",
      "Epoch 55/100\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 4.1019 - val_loss: 75.8711\n",
      "Epoch 56/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 3.9000 - val_loss: 76.2999\n",
      "Epoch 57/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 3.7224 - val_loss: 76.5405\n",
      "Epoch 58/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 3.5714 - val_loss: 76.9949\n",
      "Epoch 59/100\n",
      "6000/6000 [==============================] - 1s 114us/step - loss: 3.4284 - val_loss: 77.2715\n",
      "Epoch 60/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 3.2967 - val_loss: 77.4908\n",
      "Epoch 61/100\n",
      "6000/6000 [==============================] - 1s 115us/step - loss: 3.1662 - val_loss: 77.9153\n",
      "Epoch 62/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 2.9971 - val_loss: 78.2754\n",
      "Epoch 63/100\n",
      "6000/6000 [==============================] - 1s 115us/step - loss: 2.8344 - val_loss: 78.4834\n",
      "Epoch 64/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 2.7032 - val_loss: 78.7639\n",
      "Epoch 65/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 2.5872 - val_loss: 79.1344\n",
      "Epoch 66/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 2.4626 - val_loss: 79.2866\n",
      "Epoch 67/100\n",
      "6000/6000 [==============================] - 1s 113us/step - loss: 2.3460 - val_loss: 79.4767\n",
      "Epoch 68/100\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 2.2820 - val_loss: 79.8348\n",
      "Epoch 69/100\n",
      "6000/6000 [==============================] - 1s 114us/step - loss: 2.2165 - val_loss: 80.1233\n",
      "Epoch 70/100\n",
      "6000/6000 [==============================] - ETA: 0s - loss: 2.117 - 1s 114us/step - loss: 2.1243 - val_loss: 80.3051\n",
      "Epoch 71/100\n",
      "6000/6000 [==============================] - 1s 124us/step - loss: 2.0375 - val_loss: 80.5150\n",
      "Epoch 72/100\n",
      "6000/6000 [==============================] - 1s 115us/step - loss: 1.9294 - val_loss: 80.7985\n",
      "Epoch 73/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 1.8759 - val_loss: 81.1687\n",
      "Epoch 74/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 1.8214 - val_loss: 81.2505\n",
      "Epoch 75/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 1.7370 - val_loss: 81.5291\n",
      "Epoch 76/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 1.6480 - val_loss: 81.6874\n",
      "Epoch 77/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 1.5794 - val_loss: 81.8226\n",
      "Epoch 78/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 1.5294 - val_loss: 82.0270\n",
      "Epoch 79/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 1.4470 - val_loss: 82.3219\n",
      "Epoch 80/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 1.3810 - val_loss: 82.5530\n",
      "Epoch 81/100\n",
      "6000/6000 [==============================] - 1s 112us/step - loss: 1.3353 - val_loss: 82.6953\n",
      "Epoch 82/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 1.2970 - val_loss: 82.9275\n",
      "Epoch 83/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 1.2832 - val_loss: 83.1097\n",
      "Epoch 84/100\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 1.2209 - val_loss: 83.1308\n",
      "Epoch 85/100\n",
      "6000/6000 [==============================] - 1s 116us/step - loss: 1.1816 - val_loss: 83.3185\n",
      "Epoch 86/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 1.1359 - val_loss: 83.5475\n",
      "Epoch 87/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 1.1426 - val_loss: 83.6971\n",
      "Epoch 88/100\n",
      "6000/6000 [==============================] - 1s 112us/step - loss: 1.0978 - val_loss: 83.8532\n",
      "Epoch 89/100\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 1.0576 - val_loss: 83.8864\n",
      "Epoch 90/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 1.0165 - val_loss: 84.0843\n",
      "Epoch 91/100\n",
      "6000/6000 [==============================] - 1s 120us/step - loss: 0.9927 - val_loss: 84.1439\n",
      "Epoch 92/100\n",
      "6000/6000 [==============================] - 1s 114us/step - loss: 0.9524 - val_loss: 84.3199\n",
      "Epoch 93/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 0.8983 - val_loss: 84.5084\n",
      "Epoch 94/100\n",
      "6000/6000 [==============================] - 1s 119us/step - loss: 0.8820 - val_loss: 84.5786\n",
      "Epoch 95/100\n",
      "6000/6000 [==============================] - 1s 121us/step - loss: 0.8739 - val_loss: 84.7874\n",
      "Epoch 96/100\n",
      "6000/6000 [==============================] - 1s 122us/step - loss: 0.8319 - val_loss: 84.7270\n",
      "Epoch 97/100\n",
      "6000/6000 [==============================] - 1s 118us/step - loss: 0.8272 - val_loss: 85.0814\n",
      "Epoch 98/100\n",
      "6000/6000 [==============================] - 1s 123us/step - loss: 0.8376 - val_loss: 84.9806\n",
      "Epoch 99/100\n",
      "6000/6000 [==============================] - 1s 125us/step - loss: 0.7969 - val_loss: 85.0742\n",
      "Epoch 100/100\n",
      "6000/6000 [==============================] - 1s 117us/step - loss: 0.7699 - val_loss: 85.2217\n",
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 3s 486us/step - loss: 194.0108 - val_loss: 60.3233\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 54.6554 - val_loss: 54.3737\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 52.1815 - val_loss: 53.7161\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 2s 363us/step - loss: 51.5013 - val_loss: 53.4708\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 51.1229 - val_loss: 53.3918\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 2s 407us/step - loss: 50.7718 - val_loss: 53.3672\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 2s 406us/step - loss: 50.4347 - val_loss: 53.3462\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 2s 391us/step - loss: 50.0801 - val_loss: 53.3437\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 2s 387us/step - loss: 49.7085 - val_loss: 53.3082\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 2s 409us/step - loss: 49.3066 - val_loss: 53.2574\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 2s 405us/step - loss: 48.8861 - val_loss: 53.2518\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 2s 379us/step - loss: 48.4348 - val_loss: 53.1703\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 2s 386us/step - loss: 47.9384 - val_loss: 53.1012\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 2s 369us/step - loss: 47.4183 - val_loss: 53.0240\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 46.8621 - val_loss: 52.9110\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 2s 398us/step - loss: 46.2456 - val_loss: 52.7581\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 2s 376us/step - loss: 45.6006 - val_loss: 52.6294\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 2s 363us/step - loss: 44.9088 - val_loss: 52.3896\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 44.2068 - val_loss: 52.2695\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 2s 356us/step - loss: 43.4767 - val_loss: 52.0442\n",
      "Epoch 21/100\n",
      "6000/6000 [==============================] - 2s 366us/step - loss: 42.7212 - val_loss: 51.9082\n",
      "Epoch 22/100\n",
      "6000/6000 [==============================] - 2s 366us/step - loss: 42.0325 - val_loss: 51.7525\n",
      "Epoch 23/100\n",
      "6000/6000 [==============================] - 2s 362us/step - loss: 41.2839 - val_loss: 51.6126\n",
      "Epoch 24/100\n",
      "6000/6000 [==============================] - 2s 363us/step - loss: 40.5548 - val_loss: 51.5625\n",
      "Epoch 25/100\n",
      "6000/6000 [==============================] - 2s 368us/step - loss: 39.8500 - val_loss: 51.4550\n",
      "Epoch 26/100\n",
      "6000/6000 [==============================] - 2s 364us/step - loss: 39.1281 - val_loss: 51.2983\n",
      "Epoch 27/100\n",
      "6000/6000 [==============================] - 2s 362us/step - loss: 38.4122 - val_loss: 51.3351\n",
      "Epoch 28/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 37.6868 - val_loss: 51.3130\n",
      "Epoch 29/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 36.9809 - val_loss: 51.2552\n",
      "Epoch 30/100\n",
      "6000/6000 [==============================] - 2s 375us/step - loss: 36.2420 - val_loss: 51.3163\n",
      "Epoch 31/100\n",
      "6000/6000 [==============================] - 2s 362us/step - loss: 35.5559 - val_loss: 51.4375\n",
      "Epoch 32/100\n",
      "6000/6000 [==============================] - 2s 372us/step - loss: 34.8521 - val_loss: 51.4455\n",
      "Epoch 33/100\n",
      "6000/6000 [==============================] - 2s 365us/step - loss: 34.1452 - val_loss: 51.5347\n",
      "Epoch 34/100\n",
      "6000/6000 [==============================] - 2s 362us/step - loss: 33.4407 - val_loss: 51.7294\n",
      "Epoch 35/100\n",
      "6000/6000 [==============================] - 2s 353us/step - loss: 32.7436 - val_loss: 51.8676\n",
      "Epoch 36/100\n",
      "6000/6000 [==============================] - 2s 358us/step - loss: 32.0632 - val_loss: 52.0945\n",
      "Epoch 37/100\n",
      "6000/6000 [==============================] - 2s 372us/step - loss: 31.3520 - val_loss: 52.2584\n",
      "Epoch 38/100\n",
      "6000/6000 [==============================] - 2s 369us/step - loss: 30.6465 - val_loss: 52.5212\n",
      "Epoch 39/100\n",
      "6000/6000 [==============================] - 2s 389us/step - loss: 29.9531 - val_loss: 52.7516\n",
      "Epoch 40/100\n",
      "6000/6000 [==============================] - 2s 372us/step - loss: 29.2386 - val_loss: 52.9642\n",
      "Epoch 41/100\n",
      "6000/6000 [==============================] - 2s 369us/step - loss: 28.5390 - val_loss: 53.4614\n",
      "Epoch 42/100\n",
      "6000/6000 [==============================] - 2s 365us/step - loss: 27.8217 - val_loss: 53.5924\n",
      "Epoch 43/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 27.1009 - val_loss: 53.9553\n",
      "Epoch 44/100\n",
      "6000/6000 [==============================] - 2s 368us/step - loss: 26.3801 - val_loss: 54.3224\n",
      "Epoch 45/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 25.6756 - val_loss: 54.7027\n",
      "Epoch 46/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 24.9393 - val_loss: 54.9193\n",
      "Epoch 47/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 24.2001 - val_loss: 55.2718\n",
      "Epoch 48/100\n",
      "6000/6000 [==============================] - 2s 364us/step - loss: 23.5173 - val_loss: 55.6346\n",
      "Epoch 49/100\n",
      "6000/6000 [==============================] - 2s 364us/step - loss: 22.8420 - val_loss: 55.9845\n",
      "Epoch 50/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 22.1215 - val_loss: 56.4675\n",
      "Epoch 51/100\n",
      "6000/6000 [==============================] - 2s 365us/step - loss: 21.4344 - val_loss: 56.8504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100\n",
      "6000/6000 [==============================] - 2s 357us/step - loss: 20.7604 - val_loss: 57.2495\n",
      "Epoch 53/100\n",
      "6000/6000 [==============================] - 2s 376us/step - loss: 20.0970 - val_loss: 57.6498\n",
      "Epoch 54/100\n",
      "6000/6000 [==============================] - 2s 377us/step - loss: 19.4336 - val_loss: 58.0280\n",
      "Epoch 55/100\n",
      "6000/6000 [==============================] - 2s 368us/step - loss: 18.7734 - val_loss: 58.5411\n",
      "Epoch 56/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 18.1579 - val_loss: 58.9424\n",
      "Epoch 57/100\n",
      "6000/6000 [==============================] - 2s 358us/step - loss: 17.5609 - val_loss: 59.5539\n",
      "Epoch 58/100\n",
      "6000/6000 [==============================] - 2s 363us/step - loss: 16.9924 - val_loss: 59.8204\n",
      "Epoch 59/100\n",
      "6000/6000 [==============================] - 2s 352us/step - loss: 16.3718 - val_loss: 60.1188\n",
      "Epoch 60/100\n",
      "6000/6000 [==============================] - 2s 370us/step - loss: 15.7808 - val_loss: 60.8808\n",
      "Epoch 61/100\n",
      "6000/6000 [==============================] - 2s 352us/step - loss: 15.2651 - val_loss: 61.0349\n",
      "Epoch 62/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 14.6545 - val_loss: 61.7093\n",
      "Epoch 63/100\n",
      "6000/6000 [==============================] - 2s 356us/step - loss: 14.1273 - val_loss: 62.1300\n",
      "Epoch 64/100\n",
      "6000/6000 [==============================] - 2s 366us/step - loss: 13.6376 - val_loss: 62.5731\n",
      "Epoch 65/100\n",
      "6000/6000 [==============================] - 2s 354us/step - loss: 13.1369 - val_loss: 62.9406\n",
      "Epoch 66/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 12.6273 - val_loss: 63.5037\n",
      "Epoch 67/100\n",
      "6000/6000 [==============================] - 2s 357us/step - loss: 12.1576 - val_loss: 63.8153\n",
      "Epoch 68/100\n",
      "6000/6000 [==============================] - 2s 347us/step - loss: 11.6726 - val_loss: 64.2611\n",
      "Epoch 69/100\n",
      "6000/6000 [==============================] - 2s 365us/step - loss: 11.2432 - val_loss: 64.6366\n",
      "Epoch 70/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 10.8176 - val_loss: 65.1424\n",
      "Epoch 71/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 10.3827 - val_loss: 65.6616\n",
      "Epoch 72/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 9.9772 - val_loss: 66.2434\n",
      "Epoch 73/100\n",
      "6000/6000 [==============================] - 2s 364us/step - loss: 9.6207 - val_loss: 66.4482\n",
      "Epoch 74/100\n",
      "6000/6000 [==============================] - 2s 357us/step - loss: 9.2238 - val_loss: 66.9908\n",
      "Epoch 75/100\n",
      "6000/6000 [==============================] - 2s 363us/step - loss: 8.8725 - val_loss: 67.4011\n",
      "Epoch 76/100\n",
      "6000/6000 [==============================] - 2s 357us/step - loss: 8.5383 - val_loss: 67.8346\n",
      "Epoch 77/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 8.1884 - val_loss: 68.0779\n",
      "Epoch 78/100\n",
      "6000/6000 [==============================] - 2s 356us/step - loss: 7.8457 - val_loss: 68.6235\n",
      "Epoch 79/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 7.5201 - val_loss: 68.9945\n",
      "Epoch 80/100\n",
      "6000/6000 [==============================] - 2s 352us/step - loss: 7.2091 - val_loss: 69.2804\n",
      "Epoch 81/100\n",
      "6000/6000 [==============================] - 2s 355us/step - loss: 6.9285 - val_loss: 69.7764\n",
      "Epoch 82/100\n",
      "6000/6000 [==============================] - 2s 356us/step - loss: 6.6738 - val_loss: 70.0156\n",
      "Epoch 83/100\n",
      "6000/6000 [==============================] - 2s 366us/step - loss: 6.4118 - val_loss: 70.4151\n",
      "Epoch 84/100\n",
      "6000/6000 [==============================] - 2s 357us/step - loss: 6.1542 - val_loss: 70.9105\n",
      "Epoch 85/100\n",
      "6000/6000 [==============================] - 2s 363us/step - loss: 5.9348 - val_loss: 71.2888\n",
      "Epoch 86/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 5.6960 - val_loss: 71.6873\n",
      "Epoch 87/100\n",
      "6000/6000 [==============================] - 2s 364us/step - loss: 5.4546 - val_loss: 71.9148\n",
      "Epoch 88/100\n",
      "6000/6000 [==============================] - 2s 359us/step - loss: 5.2135 - val_loss: 72.3614\n",
      "Epoch 89/100\n",
      "6000/6000 [==============================] - 2s 361us/step - loss: 5.0008 - val_loss: 72.7148\n",
      "Epoch 90/100\n",
      "6000/6000 [==============================] - 2s 381us/step - loss: 4.7989 - val_loss: 72.8197\n",
      "Epoch 91/100\n",
      "6000/6000 [==============================] - 2s 358us/step - loss: 4.6136 - val_loss: 73.4703\n",
      "Epoch 92/100\n",
      "6000/6000 [==============================] - 2s 380us/step - loss: 4.4453 - val_loss: 73.6189\n",
      "Epoch 93/100\n",
      "6000/6000 [==============================] - 2s 362us/step - loss: 4.2751 - val_loss: 73.9095\n",
      "Epoch 94/100\n",
      "6000/6000 [==============================] - 2s 368us/step - loss: 4.0819 - val_loss: 74.4233\n",
      "Epoch 95/100\n",
      "6000/6000 [==============================] - 2s 368us/step - loss: 3.9191 - val_loss: 74.5439\n",
      "Epoch 96/100\n",
      "6000/6000 [==============================] - 2s 366us/step - loss: 3.7465 - val_loss: 74.7793\n",
      "Epoch 97/100\n",
      "6000/6000 [==============================] - 2s 360us/step - loss: 3.5913 - val_loss: 75.1759\n",
      "Epoch 98/100\n",
      "6000/6000 [==============================] - 2s 364us/step - loss: 3.4677 - val_loss: 75.6245\n",
      "Epoch 99/100\n",
      "6000/6000 [==============================] - 2s 379us/step - loss: 3.3266 - val_loss: 75.7612\n",
      "Epoch 100/100\n",
      "6000/6000 [==============================] - 2s 362us/step - loss: 3.1784 - val_loss: 75.9633\n",
      "Train on 6000 samples, validate on 1000 samples\n",
      "Epoch 1/100\n",
      "6000/6000 [==============================] - 6s 1ms/step - loss: 1509.7571 - val_loss: 84.0365\n",
      "Epoch 2/100\n",
      "6000/6000 [==============================] - 5s 807us/step - loss: 65.3188 - val_loss: 58.9367\n",
      "Epoch 3/100\n",
      "6000/6000 [==============================] - 5s 770us/step - loss: 55.0822 - val_loss: 55.1908\n",
      "Epoch 4/100\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: 53.0361 - val_loss: 54.4470\n",
      "Epoch 5/100\n",
      "6000/6000 [==============================] - 4s 727us/step - loss: 52.4119 - val_loss: 54.1321\n",
      "Epoch 6/100\n",
      "6000/6000 [==============================] - 5s 773us/step - loss: 52.0842 - val_loss: 54.0032\n",
      "Epoch 7/100\n",
      "6000/6000 [==============================] - 5s 860us/step - loss: 51.8700 - val_loss: 53.9216\n",
      "Epoch 8/100\n",
      "6000/6000 [==============================] - 5s 822us/step - loss: 51.6918 - val_loss: 53.8592\n",
      "Epoch 9/100\n",
      "6000/6000 [==============================] - 5s 803us/step - loss: 51.5375 - val_loss: 53.8004\n",
      "Epoch 10/100\n",
      "6000/6000 [==============================] - 5s 787us/step - loss: 51.4083 - val_loss: 53.7400\n",
      "Epoch 11/100\n",
      "6000/6000 [==============================] - 4s 746us/step - loss: 51.2646 - val_loss: 53.7100\n",
      "Epoch 12/100\n",
      "6000/6000 [==============================] - 5s 841us/step - loss: 51.1328 - val_loss: 53.6845\n",
      "Epoch 13/100\n",
      "6000/6000 [==============================] - 5s 788us/step - loss: 51.0050 - val_loss: 53.6515\n",
      "Epoch 14/100\n",
      "6000/6000 [==============================] - 4s 748us/step - loss: 50.8834 - val_loss: 53.6442\n",
      "Epoch 15/100\n",
      "6000/6000 [==============================] - 5s 754us/step - loss: 50.7506 - val_loss: 53.6302\n",
      "Epoch 16/100\n",
      "6000/6000 [==============================] - 4s 727us/step - loss: 50.6217 - val_loss: 53.6125\n",
      "Epoch 17/100\n",
      "6000/6000 [==============================] - 4s 725us/step - loss: 50.4905 - val_loss: 53.6126\n",
      "Epoch 18/100\n",
      "6000/6000 [==============================] - 4s 727us/step - loss: 50.3507 - val_loss: 53.5999\n",
      "Epoch 19/100\n",
      "6000/6000 [==============================] - 4s 713us/step - loss: 50.2218 - val_loss: 53.6143\n",
      "Epoch 20/100\n",
      "6000/6000 [==============================] - 4s 729us/step - loss: 50.0792 - val_loss: 53.5858\n",
      "Epoch 21/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 49.9273 - val_loss: 53.5743\n",
      "Epoch 22/100\n",
      "6000/6000 [==============================] - 4s 733us/step - loss: 49.7781 - val_loss: 53.6053\n",
      "Epoch 23/100\n",
      "6000/6000 [==============================] - 4s 747us/step - loss: 49.6161 - val_loss: 53.5582\n",
      "Epoch 24/100\n",
      "6000/6000 [==============================] - 4s 729us/step - loss: 49.4345 - val_loss: 53.5786\n",
      "Epoch 25/100\n",
      "6000/6000 [==============================] - 4s 727us/step - loss: 49.2718 - val_loss: 53.5394\n",
      "Epoch 26/100\n",
      "6000/6000 [==============================] - 4s 735us/step - loss: 49.0811 - val_loss: 53.4974\n",
      "Epoch 27/100\n",
      "6000/6000 [==============================] - 4s 732us/step - loss: 48.8852 - val_loss: 53.4888\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 48.6828 - val_loss: 53.4551\n",
      "Epoch 29/100\n",
      "6000/6000 [==============================] - 4s 711us/step - loss: 48.4770 - val_loss: 53.4484\n",
      "Epoch 30/100\n",
      "6000/6000 [==============================] - 4s 718us/step - loss: 48.2787 - val_loss: 53.3906\n",
      "Epoch 31/100\n",
      "6000/6000 [==============================] - 4s 715us/step - loss: 48.0517 - val_loss: 53.3528\n",
      "Epoch 32/100\n",
      "6000/6000 [==============================] - 4s 709us/step - loss: 47.8226 - val_loss: 53.3099\n",
      "Epoch 33/100\n",
      "6000/6000 [==============================] - 4s 719us/step - loss: 47.5837 - val_loss: 53.2510\n",
      "Epoch 34/100\n",
      "6000/6000 [==============================] - 4s 729us/step - loss: 47.3526 - val_loss: 53.2059\n",
      "Epoch 35/100\n",
      "6000/6000 [==============================] - 4s 713us/step - loss: 47.0949 - val_loss: 53.1662\n",
      "Epoch 36/100\n",
      "6000/6000 [==============================] - 4s 718us/step - loss: 46.8222 - val_loss: 53.0806\n",
      "Epoch 37/100\n",
      "6000/6000 [==============================] - 4s 715us/step - loss: 46.5549 - val_loss: 53.0119\n",
      "Epoch 38/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 46.2965 - val_loss: 52.9224\n",
      "Epoch 39/100\n",
      "6000/6000 [==============================] - 4s 713us/step - loss: 46.0065 - val_loss: 52.8444\n",
      "Epoch 40/100\n",
      "6000/6000 [==============================] - 5s 753us/step - loss: 45.7089 - val_loss: 52.7543\n",
      "Epoch 41/100\n",
      "6000/6000 [==============================] - 4s 722us/step - loss: 45.4070 - val_loss: 52.6713\n",
      "Epoch 42/100\n",
      "6000/6000 [==============================] - 4s 714us/step - loss: 45.1132 - val_loss: 52.5544\n",
      "Epoch 43/100\n",
      "6000/6000 [==============================] - 4s 718us/step - loss: 44.8075 - val_loss: 52.5158\n",
      "Epoch 44/100\n",
      "6000/6000 [==============================] - 4s 714us/step - loss: 44.5062 - val_loss: 52.3667\n",
      "Epoch 45/100\n",
      "6000/6000 [==============================] - 4s 725us/step - loss: 44.1697 - val_loss: 52.3127\n",
      "Epoch 46/100\n",
      "6000/6000 [==============================] - 4s 716us/step - loss: 43.8535 - val_loss: 52.2058\n",
      "Epoch 47/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 43.5324 - val_loss: 52.1536\n",
      "Epoch 48/100\n",
      "6000/6000 [==============================] - 4s 718us/step - loss: 43.1853 - val_loss: 52.0220\n",
      "Epoch 49/100\n",
      "6000/6000 [==============================] - 4s 717us/step - loss: 42.8906 - val_loss: 51.9796\n",
      "Epoch 50/100\n",
      "6000/6000 [==============================] - 4s 728us/step - loss: 42.5514 - val_loss: 51.8617\n",
      "Epoch 51/100\n",
      "6000/6000 [==============================] - 4s 708us/step - loss: 42.2052 - val_loss: 51.7614\n",
      "Epoch 52/100\n",
      "6000/6000 [==============================] - 4s 709us/step - loss: 41.8835 - val_loss: 51.7115\n",
      "Epoch 53/100\n",
      "6000/6000 [==============================] - 4s 719us/step - loss: 41.5561 - val_loss: 51.6548\n",
      "Epoch 54/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 41.1971 - val_loss: 51.5803\n",
      "Epoch 55/100\n",
      "6000/6000 [==============================] - 4s 730us/step - loss: 40.8605 - val_loss: 51.5509\n",
      "Epoch 56/100\n",
      "6000/6000 [==============================] - 4s 748us/step - loss: 40.5639 - val_loss: 51.5672\n",
      "Epoch 57/100\n",
      "6000/6000 [==============================] - 4s 716us/step - loss: 40.2164 - val_loss: 51.4506\n",
      "Epoch 58/100\n",
      "6000/6000 [==============================] - 4s 724us/step - loss: 39.8846 - val_loss: 51.4461\n",
      "Epoch 59/100\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: 39.5460 - val_loss: 51.4229\n",
      "Epoch 60/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 39.2105 - val_loss: 51.3859\n",
      "Epoch 61/100\n",
      "6000/6000 [==============================] - 4s 726us/step - loss: 38.8778 - val_loss: 51.4103\n",
      "Epoch 62/100\n",
      "6000/6000 [==============================] - 4s 735us/step - loss: 38.5414 - val_loss: 51.4235\n",
      "Epoch 63/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 38.2150 - val_loss: 51.4104\n",
      "Epoch 64/100\n",
      "6000/6000 [==============================] - 4s 726us/step - loss: 37.8675 - val_loss: 51.5643\n",
      "Epoch 65/100\n",
      "6000/6000 [==============================] - 4s 715us/step - loss: 37.5564 - val_loss: 51.3371\n",
      "Epoch 66/100\n",
      "6000/6000 [==============================] - 4s 727us/step - loss: 37.1807 - val_loss: 51.4209\n",
      "Epoch 67/100\n",
      "6000/6000 [==============================] - 4s 729us/step - loss: 36.8592 - val_loss: 51.4578\n",
      "Epoch 68/100\n",
      "6000/6000 [==============================] - 4s 715us/step - loss: 36.5570 - val_loss: 51.5637\n",
      "Epoch 69/100\n",
      "6000/6000 [==============================] - 4s 716us/step - loss: 36.2469 - val_loss: 51.5136\n",
      "Epoch 70/100\n",
      "6000/6000 [==============================] - 4s 736us/step - loss: 35.8465 - val_loss: 51.5480\n",
      "Epoch 71/100\n",
      "6000/6000 [==============================] - 4s 721us/step - loss: 35.5237 - val_loss: 51.7298\n",
      "Epoch 72/100\n",
      "6000/6000 [==============================] - 4s 724us/step - loss: 35.1739 - val_loss: 51.7004\n",
      "Epoch 73/100\n",
      "6000/6000 [==============================] - 4s 719us/step - loss: 34.8591 - val_loss: 51.8057\n",
      "Epoch 74/100\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: 34.5104 - val_loss: 51.9727\n",
      "Epoch 75/100\n",
      "6000/6000 [==============================] - 4s 717us/step - loss: 34.1539 - val_loss: 51.9589\n",
      "Epoch 76/100\n",
      "6000/6000 [==============================] - 4s 723us/step - loss: 33.8021 - val_loss: 51.9640\n",
      "Epoch 77/100\n",
      "6000/6000 [==============================] - 4s 720us/step - loss: 33.4654 - val_loss: 52.1878\n",
      "Epoch 78/100\n",
      "6000/6000 [==============================] - 4s 728us/step - loss: 33.1424 - val_loss: 52.1879\n",
      "Epoch 79/100\n",
      "6000/6000 [==============================] - 4s 731us/step - loss: 32.7762 - val_loss: 52.4124\n",
      "Epoch 80/100\n",
      "6000/6000 [==============================] - 4s 716us/step - loss: 32.4060 - val_loss: 52.4044\n",
      "Epoch 81/100\n",
      "6000/6000 [==============================] - 4s 725us/step - loss: 32.0672 - val_loss: 52.4457\n",
      "Epoch 82/100\n",
      "6000/6000 [==============================] - 4s 713us/step - loss: 31.7170 - val_loss: 52.6402\n",
      "Epoch 83/100\n",
      "6000/6000 [==============================] - 4s 742us/step - loss: 31.3656 - val_loss: 52.8205\n",
      "Epoch 84/100\n",
      "6000/6000 [==============================] - 5s 790us/step - loss: 30.9833 - val_loss: 52.9069\n",
      "Epoch 85/100\n",
      "6000/6000 [==============================] - 4s 749us/step - loss: 30.6415 - val_loss: 53.0895\n",
      "Epoch 86/100\n",
      "6000/6000 [==============================] - 5s 751us/step - loss: 30.2767 - val_loss: 53.2031\n",
      "Epoch 87/100\n",
      "6000/6000 [==============================] - 4s 724us/step - loss: 29.9091 - val_loss: 53.2878\n",
      "Epoch 88/100\n",
      "6000/6000 [==============================] - 4s 727us/step - loss: 29.5459 - val_loss: 53.3799\n",
      "Epoch 89/100\n",
      "6000/6000 [==============================] - 4s 729us/step - loss: 29.1614 - val_loss: 53.5178\n",
      "Epoch 90/100\n",
      "6000/6000 [==============================] - 4s 728us/step - loss: 28.7893 - val_loss: 53.8146\n",
      "Epoch 91/100\n",
      "6000/6000 [==============================] - 5s 752us/step - loss: 28.3922 - val_loss: 53.9458\n",
      "Epoch 92/100\n",
      "6000/6000 [==============================] - 5s 761us/step - loss: 27.9973 - val_loss: 54.0121\n",
      "Epoch 93/100\n",
      "6000/6000 [==============================] - 4s 748us/step - loss: 27.6052 - val_loss: 54.1755\n",
      "Epoch 94/100\n",
      "6000/6000 [==============================] - 4s 748us/step - loss: 27.2164 - val_loss: 54.4178\n",
      "Epoch 95/100\n",
      "6000/6000 [==============================] - 4s 719us/step - loss: 26.8487 - val_loss: 54.4960\n",
      "Epoch 96/100\n",
      "6000/6000 [==============================] - 4s 743us/step - loss: 26.4713 - val_loss: 54.7929\n",
      "Epoch 97/100\n",
      "6000/6000 [==============================] - 4s 726us/step - loss: 26.1022 - val_loss: 54.9015\n",
      "Epoch 98/100\n",
      "6000/6000 [==============================] - 4s 730us/step - loss: 25.7302 - val_loss: 55.1536\n",
      "Epoch 99/100\n",
      "6000/6000 [==============================] - 4s 726us/step - loss: 25.3487 - val_loss: 55.3475\n",
      "Epoch 100/100\n",
      "6000/6000 [==============================] - 4s 729us/step - loss: 24.9699 - val_loss: 55.4899\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFAQiIgnu/wBuRZz8w/3EqwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import scipy\n",
    "import sklearn\n",
    "\n",
    "import h5py\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adamax\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "tr_loss_list,cv_loss_list,unit_list = [],[],[]\n",
    "train_x_data = scipy.io.loadmat('training_input.mat')\n",
    "train_y_data = scipy.io.loadmat('training_output_expected.mat')\n",
    "\n",
    "train_x = train_x_data['X_in']\n",
    "train_y = train_y_data['Y_out']\n",
    "\n",
    "val_x_data = scipy.io.loadmat('cv_input.mat')\n",
    "val_y_data = scipy.io.loadmat('cv_output.mat')\n",
    "\n",
    "val_x = val_x_data['X_in']\n",
    "val_y = val_y_data['Y_out']\n",
    "\n",
    "for i in range(3):\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(units=500*(i+1), input_dim=224, activation=\"relu\", kernel_initializer=\"normal\"))          #hidden Layer 1\n",
    "    model.add(Dense(units=500*(i+1), activation=\"relu\", kernel_initializer=\"normal\"))                         #hidden Layer 2\n",
    "    #model.add(Dropout(rate=0.1))\n",
    "    model.add(Dense(units=500*(i+1), activation=\"relu\", kernel_initializer=\"normal\"))                         #hidden Layer 3\n",
    "    #model.add(Dropout(rate=0.1))\n",
    "    #model.add(Dense(units=600, activation=\"relu\", kernel_initializer=\"normal\"))                        #hidden Layer 4\n",
    "    #model.add(Dense(units=1000, activation=\"relu\", kernel_initializer=\"normal\"))                       #hidden Layer 5\n",
    "    #model.add(Dense(units=1000, activation=\"relu\", kernel_initializer=\"normal\"))                       #hidden Layer 6\n",
    "    #model.add(Dense(units=1000, activation=\"relu\", kernel_initializer=\"normal\"))                       #hidden Layer 7\n",
    "    model.add(Dense(units=32, activation=\"linear\",kernel_initializer=\"normal\"))                         #output Layer\n",
    "\n",
    "    model.compile(loss='mean_squared_error', optimizer='adamax')\n",
    "    hist = model.fit(train_x, train_y, epochs=100, batch_size=640, validation_data=(val_x, val_y))\n",
    "    tr_loss_list.append(hist.history['loss'][-1])\n",
    "    cv_loss_list.append(hist.history['val_loss'][-1])\n",
    "    unit_list.append(model.layers[2].output_shape[1]) ㅡ\n",
    "\n",
    "#from keras.models import load_model\n",
    "#model.save('ofdm_NN_model_0419.h5')\n",
    "\n",
    "#val_x_data = scipy.io.loadmat('cv_input.mat')\n",
    "#val_y_data = scipy.io.loadmat('cv_output.mat')\n",
    "\n",
    "#val_x = val_x_data['X_in']\n",
    "#val_y = val_y_data['Y_out']\n",
    "\n",
    "\n",
    "#loss_and_metrics = model.evaluate(val_x, val_y, batch_size=64)\n",
    "\n",
    "#print('loss_and_metrics : ' + str(loss_and_metrics))\n",
    "\n",
    "\n",
    "\n",
    "#test_x_data = scipy.io.loadmat('test_input.mat')\n",
    "\n",
    "#test_x = test_x_data['X_in']\n",
    "#yhat = model.predict(test_x)\n",
    "#scipy.io.savemat('test_output_predicted.mat', dict([('predict', yhat)]))\n",
    "\n",
    "#yhat = model.predict(train_x)\n",
    "#scipy.io.savemat('training_output_predicted.mat', dict([('predict', yhat)]))\n",
    "\n",
    "loss_dict = {}\n",
    "for i in range(len(unit_list)) :\n",
    "    loss_dict[unit_list[i]] = [tr_loss_list[i],cv_loss_list[i]]\n",
    "    \n",
    "loss_df = pd.DataFrame(loss_dict,index=['training loss','validation loss'],)\n",
    "\n",
    "excel_writer = pd.ExcelWriter('c:\\myPyCode\\data\\loss.xlsx',engine='xlsxwriter')\n",
    "loss_df.to_excel(excel_writer,index=True)\n",
    "excel_writer.save()\n",
    "\n",
    "tr_series = pd.Series(tr_loss_list,index = unit_list)\n",
    "cv_series = pd.Series(cv_loss_list,index = unit_list)\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(tr_series, 'y', label='train loss')\n",
    "loss_ax.scatter(unit_list,tr_loss_list, c='y')\n",
    "loss_ax.plot(cv_series, 'r', label='val loss')\n",
    "loss_ax.scatter(unit_list,cv_loss_list, c='r')\n",
    "loss_ax.set_xlabel('Number of Units per Layer')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.xticks(unit_list)\n",
    "plt.yticks(tr_loss_list+cv_loss_list,fontsize=7)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy\n",
    "\n",
    "for i in range (6):\n",
    "    db = str(i*5+10)\n",
    "    in_file_name = 'test_input_'+db+'.mat'\n",
    "    test_x_data = scipy.io.loadmat(file_name)\n",
    "\n",
    "    test_x = test_x_data['X_in']\n",
    "    yhat = model.predict(test_x)\n",
    "    \n",
    "    out_file_name = 'test_output_'+db+'.mat'\n",
    "    scipy.io.savemat(out_file_name, dict([('predict', yhat)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "tr_loss_list,cv_loss_list,unit_list = [],[],[]\n",
    "\n",
    "tr_loss_list.append(hist.history['loss'][-1])\n",
    "cv_loss_list.append(hist.history['val_loss'][-1])\n",
    "unit_list.append(model.layers[2].output_shape[1])\n",
    "\n",
    "tr_series = pd.Series(tr_loss_list,index = unit_list)\n",
    "cv_series = pd.Series(cv_loss_list,index = unit_list)\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(tr_series, 'y', label='train loss')\n",
    "loss_ax.plot(cv_series, 'r', label='val loss')\n",
    "loss_ax.set_xlabel('Number of Units per Layer')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "\n",
    "tr_loss_list,cv_loss_list,layer_list = [],[],[]\n",
    "\n",
    "tr_loss_list.append(hist.history['loss'][-1])\n",
    "cv_loss_list.append(hist.history['val_loss'][-1])\n",
    "layer_list.append(len(model.layers)-1)\n",
    "\n",
    "tr_series = pd.Series(tr_loss_list,index = layer_list)\n",
    "cv_series = pd.Series(cv_loss_list,index = layer_list)\n",
    "\n",
    "fig, loss_ax = plt.subplots()\n",
    "loss_ax.plot(tr_series, 'y', label='train loss')\n",
    "loss_ax.plot(cv_series, 'r', label='val loss')\n",
    "loss_ax.set_xlabel('Number of Layers')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Units</th>\n",
       "      <th>500</th>\n",
       "      <th>1000</th>\n",
       "      <th>1500</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Loss Type</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>training loss</th>\n",
       "      <td>0.769904</td>\n",
       "      <td>3.178422</td>\n",
       "      <td>24.969938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>validation loss</th>\n",
       "      <td>85.221670</td>\n",
       "      <td>75.963327</td>\n",
       "      <td>55.489914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Units                 500        1000       1500\n",
       "Loss Type                                       \n",
       "training loss     0.769904   3.178422  24.969938\n",
       "validation loss  85.221670  75.963327  55.489914"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "%matplotlib tk\n",
    "\n",
    "loss_excel = pd.read_excel('c:\\myPyCode\\data\\loss.xlsx',index=False)\n",
    "units = list(loss_excel)\n",
    "tr_loss = list(loss_excel.iloc[0])\n",
    "cv_loss = list(loss_excel.iloc[1])\n",
    "fig, loss_ax = plt.subplots()\n",
    "\n",
    "loss_ax.plot(units,tr_loss,'y', label='train loss')\n",
    "loss_ax.scatter(units,tr_loss, c='y')\n",
    "loss_ax.plot(units,cv_loss, 'r', label='val loss')\n",
    "loss_ax.scatter(units,cv_loss, c='r')\n",
    "loss_ax.set_xlabel('Number of Units per Layer')\n",
    "loss_ax.set_ylabel('loss')\n",
    "loss_ax.legend(loc='upper left')\n",
    "\n",
    "plt.xticks(units)\n",
    "plt.yticks((tr_loss+cv_loss),fontsize=7)\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import model_from_json   # 저장된 모델과 가중치 불러오기\n",
    "\n",
    "json_file = open(\"neural_network_model_sample\", \"r\") \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close() \n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"Weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-1d7b4c035419>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mjson_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"units_1200_layer_8_channel_8.h5\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'UTF-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mloaded_model_json\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mjson_file\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0mloaded_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_from_json\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloaded_model_json\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\envs\\venv\\lib\\codecs.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n\u001b[0;32m    319\u001b[0m         \u001b[1;31m# decode input (taking the buffer into account)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 321\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconsumed\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    322\u001b[0m         \u001b[1;31m# keep undecoded input until the next call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuffer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconsumed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnicodeDecodeError\u001b[0m: 'utf-8' codec can't decode byte 0x89 in position 0: invalid start byte"
     ]
    }
   ],
   "source": [
    "from keras.models import model_from_json   # 저장된 모델과 가중치 불러오기\n",
    "import scipy    # 테스트 데이터 입력 후 결과 확인\n",
    "import scipy.io\n",
    "\n",
    "json_file = open(\"units_1200_layer_8_channel_8.h5\", \"r\", encoding='UTF-8') \n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close() \n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "\n",
    "loaded_model.load_weights(\"units_1200_layer_8_channel_8_Weights.h5\")\n",
    "\n",
    "\n",
    "base_file_name = 'test_input_channel_8_'\n",
    "\n",
    "for i in range(6) :\n",
    "    file_name = base_file_name+str(10+5*i)+'dB.mat'\n",
    "    test_x_data = scipy.io.loadmat(file_name)\n",
    "    test_x = test_x_data['X_in']\n",
    "    yhat = model.predict(test_x)\n",
    "    scipy.io.savemat('test_output_predicted_channel_8_'+str(10+5*i)+'dB.mat', dict([('predict', yhat)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "import scipy\n",
    "import numpy\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "\n",
    "import keras.utils \n",
    "from keras import utils as np_utils\n",
    "import h5py\n",
    "import scipy.io\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from keras.optimizers import Adamax\n",
    "from keras.utils import np_utils\n",
    "\n",
    "model = Sequential()\n",
    "act = keras.layers.ReLU(max_value=None, negative_slope=0.0, threshold=0.0)\n",
    "\n",
    "model.add(Dense(units=1200, input_dim=224, kernel_initializer=\"normal\"))          #hidden Layer 1\n",
    "model.add(act)\n",
    "for j in range(7):\n",
    "    model.add(Dense(units=1200, kernel_initializer=\"normal\"))                     #hidden Layer 2\n",
    "    model.add(act)     \n",
    "    if (j==2):\n",
    "        model.add(Dropout(0.1))\n",
    "    \n",
    "model.add(Dense(units=32, activation=\"linear\", kernel_initializer=\"normal\"))      #output Layer : linear\n",
    "model.compile(loss='mean_squared_error', optimizer='adamax')\n",
    "model.load_weights(\"units_1200_layer_8_channel_8_Weights.h5\")\n",
    "\n",
    "base_file_name = 'test_input_channel_8_'\n",
    "\n",
    "for i in range(6) :\n",
    "    file_name = base_file_name+str(10+5*i)+'dB.mat'\n",
    "    test_x_data = scipy.io.loadmat(file_name)\n",
    "    test_x = test_x_data['X_in']\n",
    "    yhat = model.predict(test_x)\n",
    "    scipy.io.savemat('test_output_predicted_channel_8_'+str(10+5*i)+'dB.mat', dict([('predict', yhat)]))\n",
    "    \n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
